"""
æ¥å…¥ CoinGecko ä»·æ ¼æ•°æ®
ç›®æ ‡ï¼š
  - æ‹‰å–æ¯ä¸ªåè®®å¯¹åº”ä»£å¸çš„å†å²ä»·æ ¼
  - æå–ä»·æ ¼æ—©æœŸç‰¹å¾ï¼ˆå‰90å¤©ï¼‰
  - å…³é”®ï¼šæ‰¾å‡º TVL å’Œä»·æ ¼çš„"èƒŒç¦»ä¿¡å·"
  - åˆå¹¶åˆ°å·²æœ‰ç‰¹å¾è¡¨ï¼Œé‡è·‘æ¨¡å‹çœ‹ AUC æå‡å¤šå°‘
"""

import requests
import pandas as pd
import numpy as np
import time
from pathlib import Path
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR          = Path("avax_data")
EARLY_WINDOW_DAYS = 90
REQUEST_DELAY     = 1.5   # CoinGecko å…è´¹ç‰ˆé™é€Ÿ ~30æ¬¡/åˆ†é’Ÿï¼Œä¿å®ˆç”¨1.5ç§’

# â”€â”€ Step 1: é€šè¿‡ DeFiLlama æ‹¿åˆ°æ¯ä¸ªåè®®çš„ coingecko ID â”€â”€
# DeFiLlama çš„ protocol è¯¦æƒ…æ¥å£é‡Œç›´æ¥å«æœ‰ gecko_idï¼Œçœå»æ‰‹åŠ¨åŒ¹é…

def get_gecko_id(slug: str):
    """ä» DeFiLlama æ‹¿ coingecko_id"""
    url = f"https://api.llama.fi/protocol/{slug}"
    try:
        resp = requests.get(url, timeout=15)
        if resp.status_code != 200:
            return None
        data = resp.json()
        return data.get("gecko_id") or None
    except:
        return None


def fetch_price_history(gecko_id: str, days: int = 365):
    """
    è°ƒç”¨ CoinGecko /coins/{id}/market_chart
    è¿”å› DataFrame(date, price, volume, market_cap)
    å…è´¹ç‰ˆæœ€å¤šæ‹‰ 365 å¤©
    """
    url = f"https://api.coingecko.com/api/v3/coins/{gecko_id}/market_chart"
    params = {"vs_currency": "usd", "days": days, "interval": "daily"}
    try:
        resp = requests.get(url, params=params, timeout=15)
        if resp.status_code == 429:
            print("  âš ï¸  é™é€Ÿï¼Œç­‰å¾…60ç§’...")
            time.sleep(60)
            resp = requests.get(url, params=params, timeout=15)
        if resp.status_code != 200:
            return None
        data = resp.json()

        prices     = data.get("prices", [])
        volumes    = data.get("total_volumes", [])
        market_cap = data.get("market_caps", [])

        if not prices:
            return None

        df = pd.DataFrame(prices, columns=["ts", "price"])
        df["date"]       = pd.to_datetime(df["ts"], unit="ms")
        df["volume"]     = [v[1] for v in volumes]     if volumes     else 0
        df["market_cap"] = [m[1] for m in market_cap]  if market_cap else 0
        df = df.drop(columns=["ts"]).sort_values("date").reset_index(drop=True)
        return df

    except Exception as e:
        print(f"  âš ï¸  {gecko_id} è¯·æ±‚å¤±è´¥: {e}")
        return None


def extract_price_features(df_price: pd.DataFrame,
                            df_tvl:   pd.DataFrame,
                            window_days: int = 90):
    """
    ä»ä»·æ ¼æ—¶åºæå–æ—©æœŸç‰¹å¾ï¼Œå¹¶è®¡ç®—ä»·æ ¼/TVLèƒŒç¦»åº¦
    """
    # æˆªå–æ—©æœŸçª—å£
    df_p = df_price.sort_values("date").reset_index(drop=True)
    start = df_p["date"].iloc[0]
    cutoff = start + pd.Timedelta(days=window_days)
    early_p = df_p[df_p["date"] <= cutoff].copy()

    if len(early_p) < 10:
        return None

    price  = early_p["price"].values
    volume = early_p["volume"].values
    n      = len(price)

    # â”€â”€ ä»·æ ¼åŸºç¡€ç‰¹å¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    peak_price   = price.max()
    peak_idx     = price.argmax()
    final_price  = price[-1]
    first_price  = price[0] if price[0] > 0 else 1e-10

    price_peak_day_frac    = peak_idx / n
    price_retention_at_end = final_price / (peak_price + 1e-10)

    # å…¨çª—å£ä»·æ ¼è¶‹åŠ¿ï¼ˆçº¿æ€§æ–œç‡ï¼‰
    slope_price, _, _, _, _ = stats.linregress(
        np.arange(n), price / (peak_price + 1e-10)
    )

    # ä»·æ ¼æ³¢åŠ¨æ€§
    pct_changes     = np.diff(price) / (price[:-1] + 1e-10)
    price_volatility = pct_changes.std() if len(pct_changes) > 1 else 0
    price_crash_count = int((pct_changes < -0.2).sum())  # å•æ—¥è·Œè¶…20%

    # æˆäº¤é‡è¡°å‡ï¼ˆååŠæ®µ vs å‰åŠæ®µï¼‰
    mid = n // 2
    vol_ratio = volume[mid:].mean() / (volume[:mid].mean() + 1e-10)

    # â”€â”€ æ ¸å¿ƒï¼šä»·æ ¼ vs TVL èƒŒç¦»åº¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # æŠŠä»·æ ¼å’ŒTVLå¯¹é½åˆ°åŒä¸€æ—¶é—´è½´ï¼Œè®¡ç®—ç›¸å…³æ€§å’ŒèƒŒç¦»
    divergence_score    = None
    price_leads_tvl     = None
    corr_price_tvl      = None

    if df_tvl is not None and len(df_tvl) > 10:
        df_t = df_tvl.sort_values("date").reset_index(drop=True)
        early_t = df_t[df_t["date"] <= cutoff].copy()

        if len(early_t) >= 10:
            # æŒ‰å‘¨èšåˆï¼Œå‡å°‘å™ªéŸ³
            early_p["week"] = (early_p["date"] - start).dt.days // 7
            early_t["week"] = (early_t["date"] - start).dt.days // 7

            p_weekly = early_p.groupby("week")["price"].mean()
            t_weekly = early_t.groupby("week")["tvl"].mean()

            common_weeks = p_weekly.index.intersection(t_weekly.index)
            if len(common_weeks) >= 4:
                p_vals = p_weekly[common_weeks].values
                t_vals = t_weekly[common_weeks].values

                # å½’ä¸€åŒ–åˆ° 0-1
                p_norm = (p_vals - p_vals.min()) / (p_vals.ptp() + 1e-10)
                t_norm = (t_vals - t_vals.min()) / (t_vals.ptp() + 1e-10)

                # ç›¸å…³æ€§ï¼ˆé«˜ç›¸å…³=åŒæ­¥å˜åŠ¨ï¼Œä½ç›¸å…³=èƒŒç¦»ï¼‰
                if p_vals.std() > 0 and t_vals.std() > 0:
                    corr_price_tvl = float(np.corrcoef(p_norm, t_norm)[0, 1])
                else:
                    corr_price_tvl = 0.0

                # èƒŒç¦»åº¦ï¼šä»·æ ¼ä¸‹è·Œä½†TVLè¿˜åœ¨ï¼ˆå‡ºè´§ä¿¡å·ï¼‰
                # æˆ– TVLä¸‹è·Œä½†ä»·æ ¼è¿˜æ’‘ç€ï¼ˆæ‹‰ç›˜å‡ºè´§ï¼‰
                divergence_score = float(np.mean(np.abs(p_norm - t_norm)))

                # ä»·æ ¼æ˜¯å¦é¢†å…ˆTVLä¸‹è·Œï¼ˆæ»åç›¸å…³ï¼‰
                if len(p_norm) >= 3:
                    # ä»·æ ¼(t-1) å’Œ TVL(t) çš„ç›¸å…³æ€§
                    corr_lead = float(np.corrcoef(p_norm[:-1], t_norm[1:])[0, 1])
                    price_leads_tvl = corr_lead
                else:
                    price_leads_tvl = 0.0

    return {
        "price_peak_day_frac":    price_peak_day_frac,     # ä»·æ ¼å³°å€¼å‡ºç°æ—©æ™š
        "price_retention_at_end": price_retention_at_end,  # çª—å£æœ«ä»·æ ¼ç•™å­˜ç‡
        "slope_price":            slope_price,              # ä»·æ ¼æ•´ä½“è¶‹åŠ¿
        "price_volatility":       price_volatility,         # ä»·æ ¼æ³¢åŠ¨æ€§
        "price_crash_count":      price_crash_count,        # æš´è·Œæ¬¡æ•°
        "volume_decay_ratio":     vol_ratio,                # æˆäº¤é‡è¡°å‡
        "corr_price_tvl":         corr_price_tvl,           # ä»·æ ¼TVLç›¸å…³æ€§
        "divergence_score":       divergence_score,         # èƒŒç¦»åº¦ï¼ˆå…³é”®ç‰¹å¾ï¼‰
        "price_leads_tvl":        price_leads_tvl,          # ä»·æ ¼é¢†å…ˆTVLçš„ç¨‹åº¦
    }


# â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 55)
print("Step 1: è·å– CoinGecko ID")
print("=" * 55)

df_meta = pd.read_csv(DATA_DIR / "protocols_labeled.csv")
label_map = dict(zip(df_meta["slug"], df_meta["label"]))

# åŠ è½½å·²æœ‰æ—©æœŸç‰¹å¾
df_early = pd.read_csv(DATA_DIR / "early_features.csv")
slugs_to_fetch = df_early["slug"].tolist()

gecko_map = {}
for i, slug in enumerate(slugs_to_fetch):
    gecko_id = get_gecko_id(slug)
    if gecko_id:
        gecko_map[slug] = gecko_id
    if (i + 1) % 20 == 0:
        print(f"  è¿›åº¦: {i+1}/{len(slugs_to_fetch)}, æ‰¾åˆ°gecko_id: {len(gecko_map)}")
    time.sleep(0.3)

print(f"\nâœ… æ‰¾åˆ° gecko_id çš„é¡¹ç›®: {len(gecko_map)} / {len(slugs_to_fetch)}")

# ä¿å­˜æ˜ å°„å…³ç³»
pd.DataFrame([
    {"slug": k, "gecko_id": v} for k, v in gecko_map.items()
]).to_csv(DATA_DIR / "gecko_id_map.csv", index=False)
print(f"ğŸ’¾ gecko_id_map.csv å·²ä¿å­˜\n")


print("=" * 55)
print("Step 2: æ‹‰å–å†å²ä»·æ ¼ + æå–ç‰¹å¾")
print("=" * 55)

price_features = []
failed_price = []

for i, (slug, gecko_id) in enumerate(gecko_map.items()):
    print(f"[{i+1}/{len(gecko_map)}] {slug} ({gecko_id}) ...", end=" ")

    # æ‹‰ä»·æ ¼æ•°æ®
    df_price = fetch_price_history(gecko_id, days=365)
    if df_price is None or len(df_price) < 10:
        print("âŒ ä»·æ ¼æ•°æ®ä¸è¶³")
        failed_price.append(slug)
        time.sleep(REQUEST_DELAY)
        continue

    # ä¿å­˜åŸå§‹ä»·æ ¼
    df_price["slug"] = slug
    df_price.to_csv(DATA_DIR / f"price_{slug}.csv", index=False)

    # åŠ è½½å¯¹åº”çš„TVLæ—¶åº
    tvl_path = DATA_DIR / f"tvl_{slug}.csv"
    df_tvl = pd.read_csv(tvl_path, parse_dates=["date"]) if tvl_path.exists() else None

    # æå–ä»·æ ¼ç‰¹å¾
    feats = extract_price_features(df_price, df_tvl, window_days=EARLY_WINDOW_DAYS)
    if feats is None:
        print("âŒ ç‰¹å¾æå–å¤±è´¥")
        failed_price.append(slug)
        time.sleep(REQUEST_DELAY)
        continue

    feats["slug"]  = slug
    feats["label"] = label_map.get(slug, "unknown")
    price_features.append(feats)
    print(f"âœ… èƒŒç¦»åº¦={feats.get('divergence_score', 'N/A'):.3f}" if feats.get('divergence_score') else "âœ…")

    time.sleep(REQUEST_DELAY)

print(f"\næˆåŠŸ: {len(price_features)}  å¤±è´¥: {len(failed_price)}")


print("\n" + "=" * 55)
print("Step 3: åˆå¹¶ç‰¹å¾ + é‡è·‘æ¨¡å‹")
print("=" * 55)

if not price_features:
    print("âŒ æ²¡æœ‰ä»·æ ¼ç‰¹å¾æ•°æ®ï¼Œé€€å‡º")
    exit()

df_price_feats = pd.DataFrame(price_features)

# åˆå¹¶åˆ°æ—©æœŸTVLç‰¹å¾
df_combined = df_early.merge(
    df_price_feats.drop(columns=["label"]),
    on="slug", how="inner"
)
print(f"åˆå¹¶åæ ·æœ¬æ•°: {len(df_combined)}")
print(f"æ ‡ç­¾åˆ†å¸ƒ: {df_combined['label'].value_counts().to_dict()}")

df_combined.to_csv(DATA_DIR / "combined_features.csv", index=False)
print(f"ğŸ’¾ combined_features.csv å·²ä¿å­˜\n")


# â”€â”€ å¯¹æ¯”å®éªŒï¼šTVLç‰¹å¾ alone vs TVL+ä»·æ ¼ç‰¹å¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
y  = (df_combined["label"] == "dead").astype(int)

tvl_cols = [
    "peak_day_frac", "retention_at_end", "half_ratio",
    "half_life_frac", "log_growth_multiple", "volatility",
    "slope_total", "r_squared", "rise_length_frac",
    "log_peak_tvl", "crash_count", "mean_pct_change", "data_points"
]
price_cols = [
    "price_peak_day_frac", "price_retention_at_end", "slope_price",
    "price_volatility", "price_crash_count", "volume_decay_ratio",
    "corr_price_tvl", "divergence_score", "price_leads_tvl"
]

# åªç”¨å­˜åœ¨çš„åˆ—
tvl_cols   = [c for c in tvl_cols   if c in df_combined.columns]
price_cols = [c for c in price_cols if c in df_combined.columns]

print("â”€â”€ æ¨¡å‹å¯¹æ¯”ï¼ˆéšæœºæ£®æ—ï¼Œ5æŠ˜AUCï¼‰â”€â”€")
for label, cols in [
    ("TVLç‰¹å¾ï¼ˆbaselineï¼‰",    tvl_cols),
    ("ä»·æ ¼ç‰¹å¾ï¼ˆå•ç‹¬ï¼‰",        price_cols),
    ("TVL + ä»·æ ¼ç‰¹å¾ï¼ˆåˆå¹¶ï¼‰",  tvl_cols + price_cols),
]:
    X = df_combined[cols].fillna(0)
    scores = cross_val_score(
        RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced"),
        X, y, cv=cv, scoring="roc_auc"
    )
    print(f"  {label:30s} AUC: {scores.mean():.3f} Â± {scores.std():.3f}")


# â”€â”€ èƒŒç¦»ç‰¹å¾ä¸“é¡¹åˆ†æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nâ”€â”€ èƒŒç¦»åº¦ï¼ˆdivergence_scoreï¼‰åˆ†æ â”€â”€")
if "divergence_score" in df_combined.columns:
    alive = df_combined[df_combined["label"] == "alive"]["divergence_score"].dropna()
    dead  = df_combined[df_combined["label"] == "dead"]["divergence_score"].dropna()
    print(f"  alive ä¸­ä½æ•°: {alive.median():.3f}")
    print(f"  dead  ä¸­ä½æ•°: {dead.median():.3f}")
    stat, pval = stats.mannwhitneyu(alive, dead, alternative="two-sided")
    print(f"  på€¼: {pval:.4f}  {'âœ… æ˜¾è‘—' if pval < 0.05 else 'âŒ ä¸æ˜¾è‘—'}")

print("\nâ”€â”€ ä»·æ ¼é¢†å…ˆTVLï¼ˆprice_leads_tvlï¼‰åˆ†æ â”€â”€")
if "price_leads_tvl" in df_combined.columns:
    alive = df_combined[df_combined["label"] == "alive"]["price_leads_tvl"].dropna()
    dead  = df_combined[df_combined["label"] == "dead"]["price_leads_tvl"].dropna()
    print(f"  alive ä¸­ä½æ•°: {alive.median():.3f}")
    print(f"  dead  ä¸­ä½æ•°: {dead.median():.3f}")
    stat, pval = stats.mannwhitneyu(alive, dead, alternative="two-sided")
    print(f"  på€¼: {pval:.4f}  {'âœ… æ˜¾è‘—' if pval < 0.05 else 'âŒ ä¸æ˜¾è‘—'}")

print("\n" + "=" * 55)
print("æŠŠä¸‰è¡Œ AUC å¯¹æ¯”ç»“æœå‘ç»™æˆ‘ï¼Œçœ‹ä»·æ ¼æ•°æ®å¸¦æ¥å¤šå°‘æå‡")
print("=" * 55)