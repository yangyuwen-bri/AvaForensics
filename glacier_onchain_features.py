"""
åŠŸèƒ½ï¼š
  1. é€šè¿‡åˆçº¦åœ°å€æ‹‰å–é“¾ä¸Šäº¤æ˜“é‡å†å²ï¼ˆæ¯æ—¥txæ•°ï¼‰
  2. æå–"é“¾ä¸Šæ´»è·ƒåº¦è¡°å‡"ç‰¹å¾
  3. åˆå¹¶è¿›å·²æœ‰ç‰¹å¾è¡¨ï¼Œå½¢æˆä¸‰ç»´æ•°æ®ï¼šTVL + ä»·æ ¼ + é“¾ä¸Šæ´»è·ƒåº¦
  4. é‡è·‘æ¨¡å‹ï¼Œçœ‹é“¾ä¸Šæ•°æ®å¸¦æ¥å¤šå°‘æå‡

ä½¿ç”¨å‰å‡†å¤‡ï¼š
  1. å» https://avacloud.io æ³¨å†Œå…è´¹è´¦å·
  2. åˆ›å»º API Key
  3. åœ¨ä¸‹æ–¹å¡«å…¥ä½ çš„ API Keyï¼Œæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ GLACIER_API_KEY

Glacier API æ–‡æ¡£ï¼šhttps://glacier-api.avax.network/api
"""

import os
import time
from dotenv import load_dotenv
load_dotenv()  # ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
import requests
import pandas as pd
import numpy as np
from pathlib import Path
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR = Path("avax_data")

# ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæˆ–è€…ç›´æ¥å¡«åœ¨è¿™é‡Œ
GLACIER_API_KEY = os.environ.get("GLACIER_API_KEY", "")

GLACIER_BASE    = "https://glacier-api.avax.network/v1"
AVALANCHE_CHAIN = "43114"   # Avalanche C-Chain mainnet chain ID
REQUEST_DELAY   = 0.5       # å…è´¹ç‰ˆé™é€Ÿï¼Œä¿å®ˆå¤„ç†

HEADERS = {
    "Content-Type": "application/json",
    "x-glacier-api-key": GLACIER_API_KEY,
}

EARLY_WINDOW_DAYS = 90


# â”€â”€ Glacier API å°è£… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def check_api_key():
    """éªŒè¯ API Key æ˜¯å¦æœ‰æ•ˆ"""
    url = f"{GLACIER_BASE}/chains"
    resp = requests.get(url, headers=HEADERS, timeout=10)
    if resp.status_code == 401:
        print("âŒ API Key æ— æ•ˆï¼Œè¯·å» https://avacloud.io æ³¨å†Œå¹¶åˆ›å»º Key")
        return False
    elif resp.status_code == 200:
        chains = resp.json().get("chains", [])
        avax = [c for c in chains if c.get("chainId") == AVALANCHE_CHAIN]
        print(f"âœ… API Key æœ‰æ•ˆ  |  æ‰¾åˆ° Avalanche C-Chain: {bool(avax)}")
        return True
    else:
        print(f"âš ï¸  API è¿”å› {resp.status_code}: {resp.text[:200]}")
        return False


def get_contract_transactions(
    contract_address: str,
    page_size: int = 100,
    max_pages: int = 10
) -> pd.DataFrame:
    """
    æ‹‰å–åˆçº¦åœ°å€çš„å†å²äº¤æ˜“è®°å½•
    Glacier API è¿”å›æ ¼å¼: {nativeTransaction: {...}, erc20Transfers: [...]}
    """
    url = f"{GLACIER_BASE}/chains/{AVALANCHE_CHAIN}/addresses/{contract_address}/transactions"
    params = {"pageSize": page_size}
    all_txs = []

    for page in range(max_pages):
        try:
            resp = requests.get(url, headers=HEADERS, params=params, timeout=15)
            if resp.status_code == 429:
                print("  âš ï¸  é™é€Ÿï¼Œç­‰30ç§’...")
                time.sleep(30)
                continue
            if resp.status_code == 404:
                return None
            if resp.status_code != 200:
                return None

            data = resp.json()
            txs  = data.get("transactions", [])
            all_txs.extend(txs)

            next_token = data.get("nextPageToken")
            if not next_token or len(txs) == 0:
                break
            params["pageToken"] = next_token
            time.sleep(REQUEST_DELAY)

        except Exception as e:
            print(f"  âš ï¸  è¯·æ±‚å¤±è´¥: {e}")
            break

    if not all_txs:
        return None

    rows = []
    for tx in all_txs:
        # Glacier API å®é™…ç»“æ„ï¼š{nativeTransaction: {...}, erc20Transfers: [...]}
        native = tx.get("nativeTransaction", tx)  # å…¼å®¹ä¸¤ç§ç»“æ„
        ts = native.get("blockTimestamp") or native.get("timestamp", 0)
        rows.append({
            "timestamp": ts,
            "tx_hash":   native.get("txHash", ""),
            "from_addr": native.get("from", {}).get("address", ""),
            "to_addr":   native.get("to", {}).get("address", ""),
            "value":     float(native.get("value", 0)),
            # åŒæ—¶è®°å½•æ˜¯å¦åŒ…å« erc20 æ´»åŠ¨
            "has_erc20": len(tx.get("erc20Transfers", [])) > 0,
        })

    df = pd.DataFrame(rows)
    df["date"] = pd.to_datetime(df["timestamp"], unit="s", utc=True).dt.date
    return df


def get_erc20_transfers(
    contract_address: str,
    page_size: int = 100,
    max_pages: int = 5
) -> pd.DataFrame:
    """
    æ‹‰å– ERC20 ä»£å¸è½¬è´¦è®°å½•ï¼ˆå¯¹ DeFi åè®®æ›´æœ‰æ„ä¹‰ï¼‰
    """
    url = f"{GLACIER_BASE}/chains/{AVALANCHE_CHAIN}/addresses/{contract_address}/transactions:listErc20Transfers"
    params = {"pageSize": page_size}
    all_transfers = []

    for _ in range(max_pages):
        try:
            resp = requests.get(url, headers=HEADERS, params=params, timeout=15)
            if resp.status_code not in (200, 404):
                break
            if resp.status_code == 404:
                return None

            data = resp.json()
            transfers = data.get("erc20Transfers", [])
            all_transfers.extend(transfers)

            next_token = data.get("nextPageToken")
            if not next_token:
                break
            params["pageToken"] = next_token
            time.sleep(REQUEST_DELAY)
        except:
            break

    if not all_transfers:
        return None

    rows = []
    for t in all_transfers:
        rows.append({
            "timestamp": t.get("blockTimestamp", 0),
            "from_addr": t.get("from", {}).get("address", ""),
            "to_addr":   t.get("to", {}).get("address", ""),
            "value":     float(t.get("value", 0)),
        })

    df = pd.DataFrame(rows)
    df["date"] = pd.to_datetime(df["timestamp"], unit="s", utc=True).dt.date
    return df


def get_token_holders_count(contract_address: str) -> int:
    """
    è·å–ä»£å¸æŒæœ‰è€…æ•°é‡ï¼ˆERC20ï¼‰
    æŒæœ‰è€…æ•°è¶Šå°‘ â†’ è¶Šé›†ä¸­ â†’ é£é™©è¶Šé«˜
    """
    url = f"{GLACIER_BASE}/chains/{AVALANCHE_CHAIN}/tokens/{contract_address}/holders"
    params = {"pageSize": 1}
    try:
        resp = requests.get(url, headers=HEADERS, params=params, timeout=10)
        if resp.status_code == 200:
            # nextPageTokenå­˜åœ¨è¯´æ˜æœ‰å¾ˆå¤šæŒæœ‰è€…
            data = resp.json()
            holders = data.get("holders", [])
            # ç”¨ç¬¬ä¸€é¡µä¼°ç®—ï¼šå¦‚æœåªæœ‰å‡ ä¸ªæŒæœ‰è€…ï¼Œé£é™©æé«˜
            return len(holders)
    except:
        pass
    return None


# â”€â”€ ä»é“¾ä¸Šäº¤æ˜“è®°å½•æå–æ—©æœŸç‰¹å¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_onchain_features(df_tx: pd.DataFrame, window_days: int = 90) -> dict:
    """
    ä»äº¤æ˜“è®°å½•æå–æ—©æœŸæ´»è·ƒåº¦ç‰¹å¾
    åªç”¨å‰ window_days å¤©çš„æ•°æ®
    """
    if df_tx is None or len(df_tx) < 5:
        return None

    # æŒ‰æ—¥æœŸèšåˆäº¤æ˜“é‡
    df_tx["date"] = pd.to_datetime(df_tx["date"])
    daily = df_tx.groupby("date").size().reset_index(name="tx_count")
    daily = daily.sort_values("date").reset_index(drop=True)

    start = daily["date"].iloc[0]
    cutoff = start + pd.Timedelta(days=window_days)
    early = daily[daily["date"] <= cutoff].copy()

    if len(early) < 5:
        return None

    tx = early["tx_count"].values
    n  = len(tx)

    # å³°å€¼æ´»è·ƒåº¦åŠå…¶æ—¶æœº
    peak_idx     = tx.argmax()
    peak_tx      = tx.max()
    peak_day_frac = peak_idx / n

    # æ´»è·ƒåº¦ç•™å­˜ï¼ˆçª—å£æœ«å°¾ vs å³°å€¼ï¼‰
    retention = tx[-1] / (peak_tx + 1)

    # å‰åŠæ®µ vs ååŠæ®µï¼ˆæ´»è·ƒåº¦æ˜¯å¦åœ¨è¡°å‡ï¼‰
    mid = n // 2
    half_ratio = tx[mid:].mean() / (tx[:mid].mean() + 1)

    # æ•´ä½“è¶‹åŠ¿æ–œç‡
    slope, _, _, _, _ = stats.linregress(np.arange(n), tx / (peak_tx + 1))

    # å½’é›¶å¤©æ•°ï¼ˆå•æ—¥tx=0çš„æ¯”ä¾‹ï¼‰
    zero_day_frac = (tx == 0).sum() / n

    # å”¯ä¸€æ´»è·ƒåœ°å€ï¼ˆå¦‚æœæ•°æ®é‡Œæœ‰çš„è¯ï¼‰
    unique_addrs = None
    if "from_addr" in df_tx.columns:
        early_raw = df_tx[df_tx["date"] <= cutoff]
        unique_addrs = early_raw["from_addr"].nunique()

    return {
        "onchain_peak_day_frac":  peak_day_frac,   # é“¾ä¸Šæ´»è·ƒå³°å€¼å‡ºç°æ—¶æœº
        "onchain_retention":      retention,        # æ´»è·ƒåº¦ç•™å­˜ç‡
        "onchain_half_ratio":     half_ratio,       # ååŠæ®µ/å‰åŠæ®µæ´»è·ƒåº¦
        "onchain_slope":          slope,            # æ´»è·ƒåº¦è¶‹åŠ¿
        "onchain_zero_day_frac":  zero_day_frac,    # å½’é›¶å¤©æ¯”ä¾‹
        "onchain_unique_addrs":   unique_addrs or 0,
        "onchain_total_tx":       int(tx.sum()),
    }


# â”€â”€ åˆçº¦åœ°å€æ˜ å°„ï¼ˆéœ€è¦ä»DeFiLlamaåè®®è¯¦æƒ…é‡Œè·å–ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_contract_address_from_defillama(slug: str) -> str:
    """
    ä» DeFiLlama åè®®è¯¦æƒ…é‡Œæå– Avalanche ä¸»åˆçº¦åœ°å€
    ä¼˜å…ˆçº§ï¼šaddresså­—æ®µ > chainTvls.avalanche åˆçº¦ > contractså­—æ®µ
    """
    url = f"https://api.llama.fi/protocol/{slug}"
    try:
        resp = requests.get(url, timeout=15)
        if resp.status_code != 200:
            return None
        data = resp.json()

        # 1. ç›´æ¥çš„ address å­—æ®µï¼ˆæ ¼å¼ "avax:0x..."ï¼‰
        address = data.get("address", "")
        if address:
            for part in address.split(","):
                part = part.strip()
                if "avax:" in part.lower():
                    return part.split(":")[-1].strip()

        # 2. ä» contracts å­—æ®µæ‰¾
        contracts = data.get("contracts", {})
        if "avax" in contracts:
            addrs = contracts["avax"]
            if isinstance(addrs, list) and addrs:
                return addrs[0]
            if isinstance(addrs, str):
                return addrs

        # 3. ä» chainTvls é‡Œæ‰¾ Avalanche åˆçº¦
        chain_tvls = data.get("chainTvls", {})
        for chain_key in ["Avalanche", "avalanche", "AVAX"]:
            if chain_key in chain_tvls:
                chain_data = chain_tvls[chain_key]
                # æœ‰äº›åè®®åœ¨è¿™é‡Œå­˜åˆçº¦
                if isinstance(chain_data, dict):
                    sub_contracts = chain_data.get("contracts", [])
                    if sub_contracts and isinstance(sub_contracts, list):
                        return sub_contracts[0]

    except:
        pass
    return None


# â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("=" * 55)
print("AvaForensics â€” Glacier API é“¾ä¸Šæ•°æ®æ¥å…¥")
print("=" * 55)

# Step 0: éªŒè¯ API Key
if GLACIER_API_KEY == "your_api_key_here":
    print()
    print("âš ï¸  è¯·å…ˆè®¾ç½® API Keyï¼")
    print()
    print("æ­¥éª¤ï¼š")
    print("  1. è®¿é—® https://avacloud.io")
    print("  2. æ³¨å†Œå…è´¹è´¦å· â†’ åˆ›å»º Project â†’ ç”Ÿæˆ API Key")
    print("  3. æ–¹å¼Aï¼šç›´æ¥ä¿®æ”¹æœ¬æ–‡ä»¶ç¬¬22è¡Œçš„ GLACIER_API_KEY")
    print("     æ–¹å¼Bï¼šexport GLACIER_API_KEY=ä½ çš„key  ç„¶åé‡æ–°è¿è¡Œ")
    print()
    print("æ‹¿åˆ° Key åé‡æ–°è¿è¡Œæ­¤è„šæœ¬")
    exit()

print("\nStep 0: éªŒè¯ API Key...")
if not check_api_key():
    exit()

# Step 1: è·å–åˆçº¦åœ°å€
print("\nStep 1: ä» DeFiLlama è·å–åˆçº¦åœ°å€...")
df_meta = pd.read_csv(DATA_DIR / "protocols_labeled.csv")
label_map = dict(zip(df_meta["slug"], df_meta["label"]))

# ä¼˜å…ˆå¤„ç†æœ‰ä»·å€¼çš„é¡¹ç›®ï¼ˆdeadé¡¹ç›®å³°å€¼TVL > 100ä¸‡ï¼‰
df_early = pd.read_csv(DATA_DIR / "early_features.csv")
priority_slugs = df_early[
    df_early["log_peak_tvl"] > np.log1p(1e6)
]["slug"].tolist()[:50]  # å…ˆè·‘å‰50ä¸ªï¼ŒéªŒè¯æµç¨‹

print(f"ä¼˜å…ˆå¤„ç† {len(priority_slugs)} ä¸ªé«˜å³°å€¼é¡¹ç›®")

address_map = {}
for i, slug in enumerate(priority_slugs):
    addr = get_contract_address_from_defillama(slug)
    if addr:
        address_map[slug] = addr
    if (i + 1) % 10 == 0:
        print(f"  è¿›åº¦: {i+1}/{len(priority_slugs)}, æ‰¾åˆ°åœ°å€: {len(address_map)}")
    time.sleep(0.3)

print(f"âœ… æ‰¾åˆ°åˆçº¦åœ°å€: {len(address_map)} / {len(priority_slugs)}")
pd.DataFrame([
    {"slug": k, "contract_address": v} for k, v in address_map.items()
]).to_csv(DATA_DIR / "contract_addresses.csv", index=False)


# Step 2: æ‹‰é“¾ä¸Šäº¤æ˜“æ•°æ® + æå–ç‰¹å¾
print("\nStep 2: æ‹‰å–é“¾ä¸Šäº¤æ˜“è®°å½• + æå–æ´»è·ƒåº¦ç‰¹å¾...")
onchain_features = []
failed = []

for i, (slug, addr) in enumerate(address_map.items()):
    print(f"[{i+1}/{len(address_map)}] {slug[:30]:30s} ({addr[:10]}...)", end=" ")

    # å…ˆè¯• ERC20 è½¬è´¦ï¼ˆå¯¹ DeFi æ›´æœ‰æ„ä¹‰ï¼‰
    df_tx = get_erc20_transfers(addr, max_pages=3)

    # å¦‚æœæ²¡æœ‰ ERC20 è½¬è´¦ï¼Œç”¨æ™®é€š tx
    if df_tx is None or len(df_tx) < 5:
        df_tx = get_contract_transactions(addr, max_pages=3)

    feats = extract_onchain_features(df_tx, window_days=EARLY_WINDOW_DAYS)

    if feats is None:
        print("âŒ æ•°æ®ä¸è¶³")
        failed.append(slug)
        time.sleep(REQUEST_DELAY)
        continue

    feats["slug"]  = slug
    feats["label"] = label_map.get(slug, "unknown")
    onchain_features.append(feats)
    print(f"âœ… total_tx={feats['onchain_total_tx']:,}  retention={feats['onchain_retention']:.3f}")
    time.sleep(REQUEST_DELAY)

print(f"\næˆåŠŸ: {len(onchain_features)}  å¤±è´¥: {len(failed)}")


# Step 3: ç‰¹å¾åˆ†æ + åˆå¹¶æ¨¡å‹
if not onchain_features:
    print("âŒ æ²¡æœ‰é“¾ä¸Šç‰¹å¾æ•°æ®ï¼Œæ£€æŸ¥ API Key å’Œåˆçº¦åœ°å€")
    exit()

df_onchain = pd.DataFrame(onchain_features)
df_onchain.to_csv(DATA_DIR / "onchain_features.csv", index=False)
print(f"ğŸ’¾ onchain_features.csv å·²ä¿å­˜\n")

# åŒºåˆ†åŠ›åˆ†æ
print("=" * 55)
print("é“¾ä¸Šç‰¹å¾åŒºåˆ†åŠ›åˆ†æ")
print("=" * 55)
alive = df_onchain[df_onchain["label"] == "alive"]
dead  = df_onchain[df_onchain["label"] == "dead"]

onchain_cols = [c for c in df_onchain.columns
                if c.startswith("onchain_") and c not in ["slug", "label"]]

for col in onchain_cols:
    a = alive[col].dropna()
    d = dead[col].dropna()
    if len(a) < 3 or len(d) < 3:
        continue
    _, pval = stats.mannwhitneyu(a, d, alternative="two-sided")
    sig = "âœ…" if pval < 0.05 else "âŒ"
    print(f"  {col:35s}  alive={a.median():.3f}  dead={d.median():.3f}  p={pval:.4f} {sig}")

# åˆå¹¶ TVL + é“¾ä¸Š ç‰¹å¾è·‘æ¨¡å‹
print("\n" + "=" * 55)
print("æ¨¡å‹å¯¹æ¯”ï¼šTVL alone vs TVL + é“¾ä¸Šæ´»è·ƒåº¦")
print("=" * 55)

df_tvl_feats = pd.read_csv(DATA_DIR / "early_features.csv")
df_combined  = df_tvl_feats.merge(
    df_onchain.drop(columns=["label"]), on="slug", how="inner"
)
print(f"åˆå¹¶åæ ·æœ¬: {len(df_combined)}  (dead: {(df_combined['label']=='dead').sum()})")

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score

y  = (df_combined["label"] == "dead").astype(int)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

tvl_cols = [
    "peak_day_frac", "retention_at_end", "half_ratio", "half_life_frac",
    "log_growth_multiple", "volatility", "slope_total", "r_squared",
    "rise_length_frac", "log_peak_tvl", "crash_count", "mean_pct_change"
]
tvl_cols     = [c for c in tvl_cols if c in df_combined.columns]
onchain_cols = [c for c in onchain_cols if c in df_combined.columns]

for label, cols in [
    ("TVLç‰¹å¾ï¼ˆbaselineï¼‰         ", tvl_cols),
    ("é“¾ä¸Šæ´»è·ƒåº¦ï¼ˆå•ç‹¬ï¼‰            ", onchain_cols),
    ("TVL + é“¾ä¸Šæ´»è·ƒåº¦ï¼ˆåˆå¹¶ï¼‰      ", tvl_cols + onchain_cols),
]:
    X      = df_combined[cols].fillna(0)
    scores = cross_val_score(
        RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced"),
        X, y, cv=cv, scoring="roc_auc"
    )
    print(f"  {label}  AUC: {scores.mean():.3f} Â± {scores.std():.3f}")

print("\n" + "=" * 55)
print("é“¾ä¸Šç»„ä»¶æ¥å…¥å®Œæˆ âœ…")
print("è¿™æ˜¯ AvaForensics çš„ Avalanche on-chain component")
print("=" * 55)