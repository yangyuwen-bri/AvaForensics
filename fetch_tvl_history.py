"""
æ‹‰å– Avalanche åè®®çš„å†å² TVL æ—¶åºæ•°æ®
ç›®æ ‡ï¼š
  - è¿‡æ»¤æ‰ CEX ç­‰å™ªéŸ³
  - æ‹‰å–æ¯ä¸ªåè®®çš„å†å² TVL æ›²çº¿
  - åŒºåˆ†"å­˜æ´»"vs"æ­»äº¡"é¡¹ç›®
  - ä¿å­˜ä¾›åç»­ç‰¹å¾å·¥ç¨‹ä½¿ç”¨
"""

import requests
import pandas as pd
import time
import json
from pathlib import Path

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUTPUT_DIR = Path("avax_data")
OUTPUT_DIR.mkdir(exist_ok=True)

# è¿‡æ»¤æ‰è¿™äº› categoryï¼ˆCEXã€RWAç­‰ä¸é“¾ä¸ŠDeFiæ— å…³ï¼‰
EXCLUDE_CATEGORIES = {
    "CEX", "RWA", "Cross Chain Bridge", "Payments",
    "Bridge",  # è·¨é“¾æ¡¥TVLå¤§å¤šæ˜¯å¤šé“¾å…±äº«ï¼Œä¸åæ˜ Avalancheæœ¬èº«
}

# TVL ä½äºæ­¤å€¼è§†ä¸º"å®é™…æ­»äº¡"ï¼ˆç¾å…ƒï¼‰
DEAD_TVL_THRESHOLD = 10_000

# æ‹‰å–æ—¶åºæ—¶çš„è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…è¢«é™æµ
REQUEST_DELAY = 0.5

# â”€â”€ åŠ è½½åè®®åˆ—è¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = pd.read_csv("avax_protocols.csv")

# è¿‡æ»¤ CEX ç­‰å™ªéŸ³
df_clean = df[~df["category"].isin(EXCLUDE_CATEGORIES)].copy()
print(f"è¿‡æ»¤ååè®®æ•°: {len(df_clean)}  (åŸå§‹: {len(df)})")

# æ‹†åˆ†ï¼šå­˜æ´» vs æ­»äº¡
df_alive = df_clean[df_clean["tvl_usd"] >= DEAD_TVL_THRESHOLD].copy()
df_dead  = df_clean[df_clean["tvl_usd"] <  DEAD_TVL_THRESHOLD].copy()

print(f"  å­˜æ´»é¡¹ç›® (TVL >= ${DEAD_TVL_THRESHOLD:,}): {len(df_alive)}")
print(f"  æ­»äº¡/åƒµå°¸é¡¹ç›® (TVL < ${DEAD_TVL_THRESHOLD:,}): {len(df_dead)}")
print()

# ç»™é¡¹ç›®æ‰“æ ‡ç­¾
df_clean["label"] = (df_clean["tvl_usd"] >= DEAD_TVL_THRESHOLD).map(
    {True: "alive", False: "dead"}
)
df_clean.to_csv(OUTPUT_DIR / "protocols_labeled.csv", index=False)
print(f"ğŸ’¾ å·²ä¿å­˜å¸¦æ ‡ç­¾çš„åè®®åˆ—è¡¨ â†’ {OUTPUT_DIR}/protocols_labeled.csv")

# â”€â”€ æ‹‰å–å†å² TVL æ—¶åº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_tvl_history(slug: str):
    """
    è°ƒç”¨ DeFiLlama API æ‹‰å–å•ä¸ªåè®®çš„å†å² TVL
    è¿”å› DataFrame(date, tvl) æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    url = f"https://api.llama.fi/protocol/{slug}"
    try:
        resp = requests.get(url, timeout=15)
        if resp.status_code != 200:
            return None
        data = resp.json()

        # tvlByChain é‡Œæœ‰æŒ‰é“¾åˆ†å¼€çš„æ•°æ®ï¼Œä¼˜å…ˆå– Avalanche çš„
        chain_tvls = data.get("chainTvls", {})
        avax_key = None
        for key in chain_tvls:
            if "avalanche" in key.lower():
                avax_key = key
                break

        if avax_key and chain_tvls[avax_key].get("tvl"):
            records = chain_tvls[avax_key]["tvl"]
        elif data.get("tvl"):
            # æ²¡æœ‰æŒ‰é“¾åˆ†å¼€çš„å°±ç”¨æ€» TVL
            records = data["tvl"]
        else:
            return None

        df_tvl = pd.DataFrame(records)  # columns: date(timestamp), totalLiquidityUSD
        df_tvl = df_tvl.rename(columns={"totalLiquidityUSD": "tvl"})
        df_tvl["date"] = pd.to_datetime(df_tvl["date"], unit="s")
        df_tvl = df_tvl.sort_values("date").reset_index(drop=True)
        return df_tvl

    except Exception as e:
        print(f"  âš ï¸  {slug} è¯·æ±‚å¤±è´¥: {e}")
        return None


def compute_features(slug: str, df_tvl: pd.DataFrame) -> dict:
    """
    ä»æ—¶åº TVL æ•°æ®æå–é¡¹ç›®å¥åº·åº¦ç‰¹å¾
    """
    tvl = df_tvl["tvl"].values
    dates = df_tvl["date"].values

    peak_tvl = float(tvl.max()) if len(tvl) > 0 else 0
    current_tvl = float(tvl[-1]) if len(tvl) > 0 else 0
    lifespan_days = int((dates[-1] - dates[0]) / 1e9 / 86400) if len(dates) > 1 else 0

    # ä»å³°å€¼è·Œè½å¤šå°‘
    drawdown_from_peak = (
        (peak_tvl - current_tvl) / peak_tvl if peak_tvl > 0 else 0
    )

    # æœ€è¿‘30å¤© TVL å˜åŒ–ç‡
    recent = df_tvl[df_tvl["date"] >= df_tvl["date"].max() - pd.Timedelta(days=30)]
    if len(recent) >= 2:
        tvl_30d_change = (recent["tvl"].iloc[-1] - recent["tvl"].iloc[0]) / (recent["tvl"].iloc[0] + 1)
    else:
        tvl_30d_change = 0.0

    # æœ€è¿‘90å¤© TVL å˜åŒ–ç‡
    recent90 = df_tvl[df_tvl["date"] >= df_tvl["date"].max() - pd.Timedelta(days=90)]
    if len(recent90) >= 2:
        tvl_90d_change = (recent90["tvl"].iloc[-1] - recent90["tvl"].iloc[0]) / (recent90["tvl"].iloc[0] + 1)
    else:
        tvl_90d_change = 0.0

    # TVL æ³¢åŠ¨æ€§ï¼ˆæ ‡å‡†å·®/å‡å€¼ï¼‰
    mean_tvl = float(tvl.mean()) if len(tvl) > 0 else 0
    std_tvl  = float(tvl.std())  if len(tvl) > 1 else 0
    volatility = std_tvl / mean_tvl if mean_tvl > 0 else 0

    # è¿ç»­ä¸‹è·Œå¤©æ•°ï¼ˆä»æœ€é«˜ç‚¹ä¹‹åï¼‰
    peak_idx = tvl.argmax()
    post_peak = tvl[peak_idx:]
    consecutive_decline = int(sum(1 for i in range(1, len(post_peak)) if post_peak[i] < post_peak[i-1]))

    return {
        "slug": slug,
        "peak_tvl": peak_tvl,
        "current_tvl": current_tvl,
        "drawdown_from_peak": drawdown_from_peak,
        "tvl_30d_change": tvl_30d_change,
        "tvl_90d_change": tvl_90d_change,
        "volatility": volatility,
        "lifespan_days": lifespan_days,
        "consecutive_decline_days": consecutive_decline,
        "data_points": len(df_tvl),
    }


# â”€â”€ ä¸»å¾ªç¯ï¼šæ‹‰æ‰€æœ‰åè®®çš„æ—¶åºæ•°æ® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nå¼€å§‹æ‹‰å–å†å² TVL æ—¶åºï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
print("-" * 50)

all_features = []
saved_count = 0
failed = []

# ä¼˜å…ˆæ‹‰ avax_only çš„é¡¹ç›®ï¼Œå†æ‹‰å¤šé“¾çš„
df_priority = pd.concat([
    df_clean[df_clean["avax_only"] == True],
    df_clean[df_clean["avax_only"] == False],
]).reset_index(drop=True)

for i, row in df_priority.iterrows():
    slug = row["slug"]
    label = row["label"]
    print(f"[{i+1}/{len(df_priority)}] {row['name']} ({label}) ...", end=" ")

    df_tvl = fetch_tvl_history(slug)

    if df_tvl is None or len(df_tvl) < 7:
        print("âŒ æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
        failed.append(slug)
        time.sleep(REQUEST_DELAY)
        continue

    # ä¿å­˜åŸå§‹æ—¶åº
    df_tvl["slug"] = slug
    df_tvl["label"] = label
    df_tvl.to_csv(OUTPUT_DIR / f"tvl_{slug}.csv", index=False)
    saved_count += 1

    # æå–ç‰¹å¾
    features = compute_features(slug, df_tvl)
    features["label"] = label
    features["name"] = row["name"]
    features["category"] = row["category"]
    features["avax_only"] = row["avax_only"]
    all_features.append(features)

    peak = features["peak_tvl"]
    dd = features["drawdown_from_peak"]
    print(f"âœ… å³°å€¼TVL=${peak:,.0f}, è·Œå¹…={dd:.1%}")

    time.sleep(REQUEST_DELAY)

# â”€â”€ æ±‡æ€»ç‰¹å¾è¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 50)
print(f"å®Œæˆï¼æˆåŠŸ: {saved_count}ï¼Œå¤±è´¥: {len(failed)}")

if all_features:
    df_features = pd.DataFrame(all_features)
    df_features.to_csv(OUTPUT_DIR / "features_summary.csv", index=False)
    print(f"ğŸ’¾ ç‰¹å¾æ±‡æ€» â†’ {OUTPUT_DIR}/features_summary.csv")

    print("\nâ”€â”€ æ­»äº¡ vs å­˜æ´» é¡¹ç›®çš„ç‰¹å¾å¯¹æ¯” â”€â”€")
    cols = ["peak_tvl", "drawdown_from_peak", "tvl_30d_change",
            "tvl_90d_change", "lifespan_days", "volatility"]
    print(df_features.groupby("label")[cols].median().round(3).to_string())

    print("\nâ”€â”€ æ­»äº¡é¡¹ç›® category åˆ†å¸ƒ â”€â”€")
    print(df_features[df_features["label"] == "dead"]["category"].value_counts().head(10))

if failed:
    with open(OUTPUT_DIR / "failed_slugs.txt", "w") as f:
        f.write("\n".join(failed))
    print(f"\nâš ï¸  å¤±è´¥çš„ slug å·²ä¿å­˜ â†’ {OUTPUT_DIR}/failed_slugs.txt")

print("\nä¸‹ä¸€æ­¥ï¼šæŠŠ features_summary.csv å‘ç»™æˆ‘ï¼Œå¼€å§‹ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹è®­ç»ƒï¼")