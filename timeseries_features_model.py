"""
æ—¶åºç‰¹å¾æå– + Baseline æ¨¡å‹
æ ¸å¿ƒé€»è¾‘ï¼š
  - åªç”¨é¡¹ç›®"æ—©æœŸçª—å£"ï¼ˆå‰Nå¤©ï¼‰çš„TVLæ›²çº¿æå–ç‰¹å¾
  - é¢„æµ‹é¡¹ç›®æœ€ç»ˆæ˜¯å¦æ­»äº¡
  - è·‘ baseline çœ‹å‡†ç¡®ç‡ï¼Œè¯„ä¼°å“ªäº›æ—©æœŸä¿¡å·æœ€æœ‰é¢„æµ‹åŠ›
"""

import pandas as pd
import numpy as np
from pathlib import Path
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR = Path("avax_data")
EARLY_WINDOW_DAYS = 90   # åªçœ‹å‰90å¤©
MIN_DATA_POINTS   = 30   # è‡³å°‘è¦æœ‰30ä¸ªæ•°æ®ç‚¹æ‰çº³å…¥åˆ†æ
PREDICT_HORIZON   = 365  # é¢„æµ‹1å¹´åæ˜¯å¦æ­»äº¡

# â”€â”€ ä»å•æ¡æ—¶åºæå–æ—©æœŸç‰¹å¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_early_features(df_tvl: pd.DataFrame, window_days: int = 90):
    """
    è¾“å…¥ï¼šå®Œæ•´çš„TVLæ—¶åº DataFrameï¼ˆå« date, tvl åˆ—ï¼‰
    è¾“å‡ºï¼šåªç”¨å‰ window_days å¤©æ•°æ®è®¡ç®—å‡ºçš„ç‰¹å¾å­—å…¸
    """
    df = df_tvl.sort_values("date").reset_index(drop=True)
    if len(df) < MIN_DATA_POINTS:
        return None

    # æˆªå–æ—©æœŸçª—å£
    start_date = df["date"].iloc[0]
    cutoff     = start_date + pd.Timedelta(days=window_days)
    early      = df[df["date"] <= cutoff].copy()

    if len(early) < 10:
        return None

    tvl    = early["tvl"].values
    n      = len(tvl)
    t      = np.arange(n)  # æ—¶é—´è½´ï¼ˆç´¢å¼•ï¼‰

    # â”€â”€ 1. å³°å€¼ç›¸å…³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    peak_val   = tvl.max()
    peak_idx   = tvl.argmax()
    peak_day   = peak_idx  # å³°å€¼å‡ºç°åœ¨ç¬¬å‡ ä¸ªæ•°æ®ç‚¹ï¼ˆè¿‘ä¼¼å¤©æ•°ï¼‰
    peak_frac  = peak_idx / n  # å³°å€¼å‡ºç°åœ¨çª—å£çš„å“ªä¸ªä½ç½®ï¼ˆ0=æœ€å¼€å§‹, 1=æœ€æœ«å°¾ï¼‰

    # æœ€ç»ˆå€¼
    final_val  = tvl[-1]
    first_val  = tvl[0] if tvl[0] > 0 else 1

    # ç›¸å¯¹å³°å€¼è¿˜å‰©å¤šå°‘
    retention_at_end = final_val / peak_val if peak_val > 0 else 0

    # â”€â”€ 2. è¡°é€€é€Ÿåº¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    post_peak  = tvl[peak_idx:]

    # ä»å³°å€¼è·Œåˆ°50%ç”¨äº†å¤šå°‘æ­¥
    half_peak  = peak_val * 0.5
    steps_to_half = next(
        (i for i, v in enumerate(post_peak) if v <= half_peak),
        len(post_peak)  # å¦‚æœæ²¡è·Œåˆ°ï¼Œå°±ç”¨çª—å£é•¿åº¦
    )
    half_life_frac = steps_to_half / (n - peak_idx + 1)  # å½’ä¸€åŒ–

    # å³°å€¼åå‡åŒ€ä¸‹è·Œé€Ÿåº¦ï¼ˆçº¿æ€§æ‹Ÿåˆæ–œç‡ï¼‰
    if len(post_peak) >= 3:
        slope_post, _, _, _, _ = stats.linregress(
            np.arange(len(post_peak)), post_peak / (peak_val + 1)
        )
    else:
        slope_post = 0.0

    # â”€â”€ 3. æ•´ä½“å¢é•¿/è¡°é€€å½¢æ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # å…¨çª—å£çº¿æ€§è¶‹åŠ¿
    slope_total, intercept, r_val, _, _ = stats.linregress(t, tvl / (peak_val + 1))
    r_squared = r_val ** 2

    # å‰åŠæ®µ vs ååŠæ®µçš„å‡å€¼å¯¹æ¯”ï¼ˆåˆ¤æ–­æ•´ä½“æ˜¯åœ¨ä¸Šæ¶¨è¿˜æ˜¯ä¸‹è·Œï¼‰
    mid = n // 2
    first_half_mean  = tvl[:mid].mean()
    second_half_mean = tvl[mid:].mean()
    half_ratio = second_half_mean / (first_half_mean + 1)  # >1 ä¸Šæ¶¨ï¼Œ<1 ä¸‹è·Œ

    # â”€â”€ 4. æ³¢åŠ¨æ€§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # æ—¥æ¶¨è·Œå¹…
    pct_changes = np.diff(tvl) / (tvl[:-1] + 1)
    volatility      = pct_changes.std() if len(pct_changes) > 1 else 0
    mean_pct_change = pct_changes.mean() if len(pct_changes) > 1 else 0

    # æç«¯æš´è·Œæ¬¡æ•°ï¼ˆå•æ—¥è·Œè¶…30%ï¼‰
    crash_count = int((pct_changes < -0.3).sum())

    # â”€â”€ 5. å¢é•¿é˜¶æ®µç‰¹å¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ä¸Šæ¶¨æœŸé•¿åº¦ï¼ˆå³°å€¼å‰ï¼‰
    rise_length_frac = peak_idx / n

    # åˆå§‹å¢é€Ÿï¼ˆç¬¬ä¸€ä¸ªæ•°æ®ç‚¹åˆ°å³°å€¼çš„å¢é•¿å€æ•°ï¼‰
    growth_multiple = peak_val / (first_val + 1)
    log_growth      = np.log1p(growth_multiple)

    # â”€â”€ 6. è§„æ¨¡ç‰¹å¾ï¼ˆå¯¹æ•°å˜æ¢ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log_peak_tvl  = np.log1p(peak_val)
    log_final_tvl = np.log1p(final_val)

    return {
        # å³°å€¼ç›¸å…³
        "peak_day_frac":      peak_frac,          # å³°å€¼å‡ºç°æ—©æ™šï¼ˆè¶Šæ—©=è¶Šå¯èƒ½æ˜¯åºæ°ï¼‰
        "retention_at_end":   retention_at_end,   # çª—å£æœ«å°¾è¿˜å‰©å³°å€¼çš„å¤šå°‘
        "half_life_frac":     half_life_frac,      # è·Œåˆ°ä¸€åŠå³°å€¼çš„é€Ÿåº¦ï¼ˆè¶Šæ…¢è¶Šå¥½ï¼‰

        # è¡°é€€æ–œç‡
        "slope_post_peak":    slope_post,          # å³°å€¼åä¸‹è·Œæ–œç‡ï¼ˆè´Ÿ=ä¸‹è·Œï¼‰
        "slope_total":        slope_total,         # å…¨çª—å£è¶‹åŠ¿

        # å½¢æ€
        "half_ratio":         half_ratio,          # ååŠæ®µ/å‰åŠæ®µå‡å€¼ï¼ˆ>1=æ•´ä½“ä¸Šæ¶¨ï¼‰
        "r_squared":          r_squared,           # è¶‹åŠ¿æ‹Ÿåˆä¼˜åº¦

        # æ³¢åŠ¨æ€§
        "volatility":         volatility,          # æ—¥æ¶¨è·Œå¹…æ ‡å‡†å·®
        "mean_pct_change":    mean_pct_change,     # å¹³å‡æ—¥æ¶¨è·Œå¹…
        "crash_count":        crash_count,         # æš´è·Œæ¬¡æ•°

        # å¢é•¿è´¨é‡
        "rise_length_frac":   rise_length_frac,    # ä¸Šæ¶¨æœŸå æ¯”
        "log_growth_multiple": log_growth,         # åˆå§‹å¢é•¿å€æ•°ï¼ˆå¯¹æ•°ï¼‰

        # è§„æ¨¡
        "log_peak_tvl":       log_peak_tvl,        # å³°å€¼TVLï¼ˆå¯¹æ•°ï¼‰
        "data_points":        n,
    }


# â”€â”€ åŠ è½½æ‰€æœ‰æ—¶åºæ–‡ä»¶ï¼Œæå–ç‰¹å¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("åŠ è½½æ—¶åºæ•°æ®å¹¶æå–æ—©æœŸç‰¹å¾...")
print(f"çª—å£: å‰ {EARLY_WINDOW_DAYS} å¤©  |  æœ€å°æ•°æ®ç‚¹: {MIN_DATA_POINTS}")
print("-" * 50)

# åŠ è½½ label ä¿¡æ¯
df_meta = pd.read_csv(DATA_DIR / "protocols_labeled.csv")
label_map = dict(zip(df_meta["slug"], df_meta["label"]))
name_map  = dict(zip(df_meta["slug"], df_meta["name"]))
cat_map   = dict(zip(df_meta["slug"], df_meta["category"]))

records = []
skipped = 0

for fpath in sorted(DATA_DIR.glob("tvl_*.csv")):
    slug = fpath.stem.replace("tvl_", "")
    if slug not in label_map:
        continue

    df_tvl = pd.read_csv(fpath, parse_dates=["date"])
    feats = extract_early_features(df_tvl, window_days=EARLY_WINDOW_DAYS)

    if feats is None:
        skipped += 1
        continue

    feats["slug"]     = slug
    feats["label"]    = label_map[slug]
    feats["name"]     = name_map.get(slug, slug)
    feats["category"] = cat_map.get(slug, "")
    records.append(feats)

df_feats = pd.DataFrame(records)
print(f"æˆåŠŸæå–: {len(df_feats)} ä¸ªé¡¹ç›®  |  è·³è¿‡(æ•°æ®ä¸è¶³): {skipped}")
print(f"æ ‡ç­¾åˆ†å¸ƒ: {df_feats['label'].value_counts().to_dict()}")

df_feats.to_csv(DATA_DIR / "early_features.csv", index=False)
print(f"ğŸ’¾ å·²ä¿å­˜ â†’ {DATA_DIR}/early_features.csv\n")

# â”€â”€ ç‰¹å¾åŒºåˆ†åŠ›åˆ†æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 50)
print("ç‰¹å¾åŒºåˆ†åŠ›ï¼ˆMann-Whitney U æ£€éªŒï¼‰")
print("=" * 50)

feature_cols = [c for c in df_feats.columns
                if c not in ["slug", "label", "name", "category"]]

alive = df_feats[df_feats["label"] == "alive"]
dead  = df_feats[df_feats["label"] == "dead"]

rows = []
for col in feature_cols:
    a = alive[col].dropna()
    d = dead[col].dropna()
    if len(a) < 5 or len(d) < 5:
        continue
    stat, pval = stats.mannwhitneyu(a, d, alternative="two-sided")
    rows.append({
        "ç‰¹å¾": col,
        "aliveä¸­ä½æ•°": round(a.median(), 4),
        "deadä¸­ä½æ•°":  round(d.median(), 4),
        "på€¼": pval,
        "æ˜¾è‘—": "âœ…" if pval < 0.01 else ("âš ï¸" if pval < 0.05 else "âŒ")
    })

df_rank = pd.DataFrame(rows).sort_values("på€¼")
print(df_rank.to_string(index=False))

# â”€â”€ Baseline æ¨¡å‹ï¼ˆéšæœºæ£®æ—ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 50)
print("Baseline æ¨¡å‹ï¼šéšæœºæ£®æ—ï¼ˆåªç”¨æ—©æœŸç‰¹å¾ï¼‰")
print("=" * 50)

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, roc_auc_score
import sklearn

X = df_feats[feature_cols].fillna(0)
y = (df_feats["label"] == "dead").astype(int)

print(f"æ ·æœ¬æ•°: {len(X)}  |  ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"æ­»äº¡é¡¹ç›®å æ¯”: {y.mean():.1%}\n")

# 5æŠ˜äº¤å‰éªŒè¯
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for name, model in [
    ("éšæœºæ£®æ—",        RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced")),
    ("æ¢¯åº¦æå‡(GBDT)", GradientBoostingClassifier(n_estimators=200, random_state=42)),
]:
    auc_scores = cross_val_score(model, X, y, cv=cv, scoring="roc_auc")
    acc_scores = cross_val_score(model, X, y, cv=cv, scoring="accuracy")
    print(f"{name}:")
    print(f"  AUC:      {auc_scores.mean():.3f} Â± {auc_scores.std():.3f}")
    print(f"  Accuracy: {acc_scores.mean():.3f} Â± {acc_scores.std():.3f}")
    print()

# ç”¨å®Œæ•´æ•°æ®è®­ç»ƒä¸€æ¬¡ï¼Œçœ‹ç‰¹å¾é‡è¦æ€§
rf = RandomForestClassifier(n_estimators=300, random_state=42, class_weight="balanced")
rf.fit(X, y)

print("â”€â”€ ç‰¹å¾é‡è¦æ€§ï¼ˆéšæœºæ£®æ—ï¼‰â”€â”€")
importance_df = pd.DataFrame({
    "ç‰¹å¾": feature_cols,
    "é‡è¦æ€§": rf.feature_importances_
}).sort_values("é‡è¦æ€§", ascending=False)
print(importance_df.to_string(index=False))

# â”€â”€ ä¿å­˜ç‰¹å¾é‡è¦æ€§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
importance_df.to_csv(DATA_DIR / "feature_importance.csv", index=False)
print(f"\nğŸ’¾ ç‰¹å¾é‡è¦æ€§ â†’ {DATA_DIR}/feature_importance.csv")

print("\n" + "=" * 50)
print("ä¸‹ä¸€æ­¥ï¼šæŠŠç»ˆç«¯è¾“å‡ºï¼ˆç‰¹åˆ«æ˜¯AUCåˆ†æ•°å’Œç‰¹å¾é‡è¦æ€§ï¼‰å‘ç»™æˆ‘")
print("æˆ‘ä»¬æ ¹æ®ç»“æœå†³å®šï¼šè¡¥å……å“ªäº›æ–°ç‰¹å¾ / è°ƒæ•´é¢„æµ‹çª—å£ / ä¼˜åŒ–æ¨¡å‹")
print("=" * 50)